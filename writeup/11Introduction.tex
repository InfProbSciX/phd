
\chapter{Introduction}

Representation learning is a widely used set of ideas within machine learning and artificial intelligence \citep{rep_learn_review}, and has close ties to statistical modelling. It is a core step in many scientific workflows, as scientists convert raw data into features more actionable for computers and/or humans. Scientific representation learning presents practical constraints,
as prior knowledge is typically available at the outset without a universally agreed-upon strategy to incorporate it. There is a need for interpretable output, which isn't guaranteed by many methods. In this thesis, in \cref{part:i}, we show that case studies exist across drug discovery, climate science, and genetics. A variety of algorithms are used for representation learning, particularly within the biological sciences (such as t-SNE; \cite{scrna-pipeline}), with mechanisms of operation that are hard to modify systematically and lack explicit modelling \GLS{semantic}s.
Although there is a widespread effort to understand methods in the field, there is no widely adopted, unified set of explicit rules that governs how methods are to be constructed, constrained, and compared with one another. Such a rule-set would streamline problem solving and address the construction and comparability problems faced in scientific representation learning.

To address this problem, we describe the central tenet of a probabilistic paradigm, using the language of probability and statistics, that explains many methods in representation learning. The reason for our study of representation learning from a probabilistic perspective are as follows. Statistical models, when well understood, act as rough high-level grammars (in the sense of a modelling language) with which one can talk about and represent data and associated knowledge of interest. We use the term probabilistic ``grammars'' in this thesis frequently as a metaphor for compositional rule-sets for model construction. In other words, we use ``grammars'' in the sense that we can define a formal language to compile logical real-world representations to probabilistic models (using probabilistic programming languages, reviewed by \cite{intro-ppl}).

A probabilistic representation learning paradigm would make constructing scientifically-tailored methods accessible to practitioners through the high-level grammars that the models would correspond to. Moreover, probabilistic modelling fits into representation learning contexts, as we can model noise and uncertainties within data explicitly at variable scales. Probabilistic models also bring the following practical strengths. Firstly, they enable the composability of methods and model extension in light of atypical data or prior information. Secondly, they can aid identifiability because priors and models can significantly constrain the search space of algorithms. Thirdly, probabilistic modelling often narrows the objective of interest to the posterior distribution's log-density. This makes automatic inference possible, particularly through the use of probabilistic programming languages \citep{zoubin_pml, bda3}. Fourthly, probabilistic models are typically robust to changes in the problem's representation and are therefore more suitable for representing models of the world. Lastly, probabilistic models enable analyses outside their original use, for example, classifiers storing information about a point's density, discussed in \cref{sub:lda-perspective}.

Therefore, in this thesis we attempt to unify the various algorithms in scientific representation learning, particularly dimensionality reduction (DR) methods, from a probabilistic perspective, and show that such a paradigm has strengths commonly seen with probabilistic frameworks. Moreover, we use our framework in \cref{chp:probdr-nn} to suggest principled architecture modifications to transformers, suggesting that our ideas extend to unintended impactful use-cases. Concretely, in this thesis, we pose the question:
\begin{displayquote}
\textbf{What ideas and models unify representation learning from a probabilistic perspective}?
\end{displayquote}
The answer will be:
\begin{displayquote}
The studied representation learning methods \textbf{perform inference in a unified probabilistic model, acting on a minimal aspect of the data}, potentially framed variationally with a fixed variational target representing observations.
\end{displayquote}
%
An overview of the thesis is as follows. In the \textbf{background (\cref{part:i})}, we present an overview of many ideas revealing the nature of probabilistic models in representation learning, and show that practitioners typically compose models into end-to-end pipelines using high-level \GLS{semantic}s. This presentation is in line with other reviews of representation learning, for example, that of \cite{rep_learn_review, pml-ii}. We exemplify in this chapter \textbf{how} prior scientific knowledge enters modelling practice---classical Bayesian priors are not the only (or even the most common) form of knowledge input. In fact, it is the explicit estimation and holding static of domain knowledge guided quantities, that constrains models across many use-cases. In many real-world cases, we find that methods already conform to the idea that one must limit degrees of freedom of models for the inference of useful latent variables. In this chapter, we show however, that probabilistic models are not the only tool for representation learning, and that there are many \textbf{algorithms} that process data and produce outputs of downstream interest, which currently lack probabilistic interpretations. The key question posed by this chapter is, given our organisation of representation learning methods, \textbf{where do such algorithms fit?} As an example, for biological applications such as single-cell data analysis, practitioners use many dimensionality reduction algorithms without probabilistic interpretations to compress high-dimensional data points into lower-dimensional real-valued representations. The representations are for downstream problems such as cluster identification, real-valued phenotypes or causal factors, and temporal ordering within the data \citep{scrna-pipeline}. Outputs of such algorithms, and the algorithms themselves, can be highly uninterpretable and difficult to modify given new information or atypical data. In the thesis, we provide probabilistic interpretations and show concretely that these algorithms, which are widely used, are generative models for minimal statistics of the data.

In the core chapter of the thesis \textbf{(\cref{part:ii})}, to address the problem of the proliferation of algorithms that exist without comparability to known probabilistic models, we introduce the \textbf{ProbDR framework}---a probabilistic framework we develop by examining various dimensionality reduction methods used in science. It sheds light on how these methods are constrained and how they can be compared to each other. Through this framework, we observe \textit{what} representation learning methods model---an estimate of the data's covariance. Furthermore, by studying algorithms that work well in practice, we can uncover modelling strategies that work well across domains and show that they are broadly coherent. Through the ProbDR framework, we demonstrate how many modern algorithms constrain their inference: by modelling only a few aspects of the data. Therefore, specification of relatively simple models is possible without the risk of overfitting/overparameterisation (allowing for an Occam's-razor effect). In other words, through constraint imposed by the estimation of a very specific characteristic of the data, there is less misspecification, as all models, particularly latent variable models, tend to experience a mismatch between data characteristics and modelling assumptions.

A limitation of the work is that ProbDR is a \textit{derived} framework, i.e., one obtained by studying classical methods as they are, which is not without issues, simply due to the nature of the algorithms it seeks to explain. For example, most methods examined perform inference for data characteristics in such a way that induction to new, unseen data points cannot be done easily. ProbDR interpretations share this characteristic. Moreover, the studied methods recover point estimates of latent variables; we leave the sampling behaviour analysis of ProbDR to future work. Other model frameworks may underpin representation learning more clearly, that are more interpretable, and that are easier to perform inference with. Specifically, we argue that appropriately constrained GPLVMs \citep{gplvm} and edge detection models presented in \cref{part:ii} (\cref{chp:probdr-nonlin}) provide a clearer route to the development of new methods. However, ProbDR achieves the goals set for a unifying framework and is, to our knowledge, the first of its kind to explain a wide variety of methods in the field from a probabilistic modelling perspective.

In the last part of the thesis \textbf{(\cref{chp:probdr-nn})}, as a validation of the study of the methods above, we show how the characteristics of the ProbDR framework extend to representation learning more widely. This exemplifies a core characteristic of probabilistic models, that insights from one area of study typically do extend to other areas where a model is reused. We focus specifically on the transformer architecture and show potential improvements through insights gleaned from the ProbDR framework. These insights will show that \textbf{transformers derived using our ideas result in a graph diffusion} step in place of an attention-based update, a change which leads to increased performance using nanoGPT on a language task and a vision task.

In the next section, we summarise the ideas of the thesis in more detail.

\section{Thesis outline}

The thesis is split into three content chapters, the background, the framework describing dimensionality reduction, and the extensions to representation learning.

In the \textbf{background}, \cref{part:i}, a summary of the major ideas in representation learning is presented. The aim of the chapter is to show our perspective on how probabilistic methods in representation are organised, before showing that there are various algorithms that don't cleanly sit in this view. We categorise different ideas in probabilistic representation as,
\begin{itemize}
    \item \textbf{Projective methods}: Many methods of constructing representations are functions of the inputs. These include random projections, featurising functions, density estimators, and functions that act as regressors and classifiers. We present ideas relating to the usage of such methods for science, showing what probabilistic models enable. As an example, we show that confidence levels of classifiers can correspond to how in-domain or out-of-domain an input is.
    \item \textbf{Generative methods}: These methods (often probabilistic models) find latent representations that describe the statistics of observed data or the data directly, such as covariances or distance matrices. Relevant observations that can be made about the presented model classes are described: for example, they demonstrate how model misspecification can arise and lead to failure modes in optimisation.
    \item \textbf{Dimensionality reduction and neighbour embedding methods}: Many methods for dimensionality reduction are motivated by matching a matrix of high-dimensional distances using a low dimensional matrix. Other methods perform link/edge prediction on graphs, with distances between learnt latent embeddings describing the probabilities of an edge existing between two nodes in the data graph. The latter can typically be formulated as contrastive algorithms. We pose the question, where should these methods sit? In the next chapter, we show that the algorithms correspond to inference algorithms in generative models for the covariance (or more generally, data similarity measures).
\end{itemize}
%
Through case studies, this thesis demonstrates how the usage of methods across climate science use-cases can be accelerated using probabilistic programming, how conservation activities are enhanced using principles of representation learning, how observations of models made in speech can facilitate the discovery of proteins for drug-discovery, and how constraints can indicate the properties that cancer-like cells share, thereby improving biological understanding for rare-disease research. Conversely, we also see what is shared by many of these case-studies: that they constrain many aspects of their models through case-specific estimation of some quantities that are of interest.

In the next chapter, we describe the \textbf{ProbDR} framework, that explains many classical coordinate-focused dimensionality reduction methods. Coordinate-focused meaning that the methods can be seen to explicitly represent and perform full-form inference for latent coordinates/embeddings, as opposed to seeking a parametric function that performs dimensionality reduction as a function of the data. This is an emergent framework, i.e., one obtained by studying what objectives dimensionality reduction methods optimise and interpreting these objectives as inference algorithms within probabilistic models. Due to the nature of probabilistic models, we show that ``\textbf{what}'' (i.e. the random variable, or the data statistic) is modelled by various DR methods can be unified coherently, i.e., with the assumptions resulting from model specification remaining unchanged after transformation of the random variable. We show (in \cref{chp:probdr-lin} and \cref{chp:probdr-nonlin}) that every DR algorithm studied performs a variant of \textbf{covariance estimation}, within a model class where a covariance estimate $\S$ is described using a Wishart or inverse-Wishart distribution that has as the centrality parameter, a covariance kernel akin to those used with Gaussian processes,
$$ \S | \X \sim \mathcal{W}^{\{-1\}}(\X\X^T + \beta K(\X, \X) + \gamma \I_n, \nu), $$
with latent $\X$ found through maximum a-posteriori inference. All hyperparameters (e.g., $\beta, \gamma, \nu$), the choice of distribution (Wishart or inverse-Wishart) and data covariance estimators $\S(\Y)$ are fixed and known, and depend on the algorithm that is being interpreted. The central matrix $k$ corresponds to a non-linear kernel matrix, which as we shall see will be a Cauchy (rational quadratic) covariance kernel. The covariance estimator is the regular estimator $\S = \Y\Y^T/d$ only in the case of GPLVM and its extensions or simplifications, and in most cases involves instead (a pseudo-inverse of) a graph Laplacian (GL) matrix encoding a nearest-neighbour graph. We present efficient inference options inspired by SGNS \citep{sgns-svd} for some of the models, drawing connections between the two main interpretations within ProbDR that are of the form,
$$ \text{\texttt{transformed data}} \sim \text{\texttt{simple model}}$$
and,
$$\text{\texttt{raw data}} \sim \text{\texttt{complex model}}.$$
Finally, in this chapter describing ProbDR, we show that inference within the model above is equivalent to KL-minimisation, with the variational constraint over the covariance/precision parameter determined using the data, and being completely fixed at the outset of inference. This interpretation will be helpful for the last part of the thesis.

We will then focus on \textbf{extensions} in \cref{chp:probdr-nn}, showing that constructions in wider representation learning, such as some self-supervised learning algorithms and transformers, can be seen to do approximate inference within ProbDR, and share similar ideas. We show that transformers correspond to unrolled optimisation, as presented by \cite{whitebox-transformer}, with a different perspective on the model used (described in \cref{probdr-iii-res-diff}). This improves performance of the architecture---noting that in our interpretation of transformers corresponding to unrolled probabilistic Laplacian Eigenmaps, a stabilising constraint appears naturally as a graph Laplacian in the place of the classical attention matrix (interpreted as an adjacency matrix). We demonstrate that this leads to higher validation scores using nanoGPT on a language task and a vision task. This demonstrates an empirical application of the lessons learnt from the derivation of the ProbDR framework, in a setting that was not anticipated during the construction of the framework.

A visual summary of these ideas is given in \cref{fig:intro:ga}, showing that the thesis follows the plan described thus far. This is that the background presents the landscape of methods, then, we interpret the methods as maximum likelihood methods within the ProbDR framework, which have equivalent variational views, and finally show that ideas of the framework can be related to transformers, an unintended case-study for our work.
\begin{figure}[ht!]
    \centering
    \vspace{-1.25cm}
    \includegraphics[width=0.9\linewidth]{illustrations/intro_ga.pdf}
    \vspace{-1.25cm}
    \caption{A visual summary of the thesis. In the \textbf{background}, we present the landscape of large ideas in representation learning showing projective and generative methods, and equivalences between them, finally posing the question, where should neighbour embedding (and other dimensionality reduction algorithms) sit? In the \textbf{ProbDR} chapter, we show that (quasi) maximum-likelihood estimation for the covariance/precision leads to many of the algorithms in the field, with connections between them, and that this has an equivalent KL-minimisation view. We then show in the \textbf{transformers} chapter that ideas of the framework extend to SSL, and KL minimisation in probabilistic Laplacian Eigenmaps can explain the transformer architecture.}
    \label{fig:intro:ga}
\end{figure}

In the next section, we list the publications that make up the thesis to highlight the specific contributions made.

\section{Publication list and contributions}

\textbf{Major contributions}, i.e. (joint-)first and second author publications, and two papers with significant software contributions are presented below, and throughout the thesis, these are coloured grey to highlight the contributions and original material in the thesis, for example, \mecite{probdr}. Details of contributions are listed below. Code for all presented results is at \href{https://github.com/InfProbSciX/phd/}{\textbf{github:infprobscix/phd}}.

\subsection*{Case Studies in Science (mentioned in \Cref{part:i} and the Appendices)}
\begin{itemize}
    \item \mecite{my-zeroshot-bio}: Protein Language Model Zero-Shot Fitness Predictions are Improved by Inference-only Dropout, \textit{1$^{st}$ author, Workshop Track @ MLCB, 2025}. Contributions include all technical ideas, experiments, code and write-up.
    \item \mecite{proxbias}: Weakly supervised latent variable inference of proximity bias in crispr gene knockouts from single-cell images, \textit{1$^{st}$ author, LMRL @ ICLR, 2025}. Contributions include the main implementation (excluding the fine-tuning of the auto-encoder that was used and the single-cell centroid identification of RXRX3 images) and the probabilistic interpretation of the method.
    \item \mecite{monke}: On Feature Learning for Titi Monkey Activity Detection, \textit{1$^{st}$ author, VIHAR @ Interspeech 2024}. Contributions include all technical ideas, experiments and write-up. Jen Muir contributed the subject-matter expertise, experimental direction, validation and datasets. This paper is a short technical summary and experimental extension of a longer piece of work for which Eric Meissner also contributed code and the initial implementation that was a precursor to this work.
    \item \mecite{zero-shot-mos}: Uncertainty as a predictor: Leveraging self-supervised learning for zero-shot MOS prediction, \textit{1$^{st}$ author, SASB @ ICASSP 2024}. Contributions include most technical ideas, experiments, code and write-up.
    \item \mecite{sarah-insights}: Scalable Amortised GPLVMs for Single Cell Transcriptomics Data, \textit{2$^{nd}$ author, MLGenX @ ICLR 2024}. Acted as advisor and peer-coder on this project and set up a minimal script that achieved scVI performance using a linear variational GPLVM. SZ performed all ablations of this code, derived insights, and drafted the publication.
    \item \mecite{tech-gplvm}: Modelling technical and biological effects in scRNA-seq data with scalable GPLVMs, \textit{joint 1$^{st}$ author, MLCB 2022}. Contributions include main technical interpretation and secondary interpretations in the appendices, the experiment related to the replication of the results of \cite{natsuhiko-paper} and most of the software.
\end{itemize}

\subsection*{Software Papers (mentioned in \Cref{part:i} and the Appendices)}
\begin{itemize}
    \item \mecite{gplvm-svi}: Generalised GPLVM with stochastic variational inference, \textit{2$^{nd}$ author, 2022, AISTATS}. Contributions include a large part of the software, its design, proof-reading, project input, and multiple experiments (including the \texttt{mocap, mnist, pca-flow, poisson} experiments).
    \item \mecite{ice-cores}: Ice Core Dating using Probabilistic Programming, \textit{1$^{st}$ author, AGU, \& GPSMDS @ Neurips 2022}. Contributions include all of the implementation, some project steering, and drafting of the publication.
    \item \mecite{gauche}: GAUCHE: a library for Gaussian processes in chemistry, \textit{joint 1$st$ author, NeurIPS 2023}. Contributions include a wrapper that makes external graph kernels usable via plug-and-play, and two experiments (that perform scalar property prediction using the wrapper, and another experiment that uses \texttt{torch}'s Weisfeiler-Lehman feature function).
    \item \mecite{geomkerns}: The GeometricKernels Package: Heat and Mat\'ern Kernels for Geometric Learning on Manifolds, Meshes, and Graphs, \textit{JMLR, 2025}. Contributions include the implementation of kernels on graphs and the \texttt{scipy.sparse} interfacing.
\end{itemize}

\subsection*{Core theoretical contributions (these form \Cref{part:ii}: ProbDR)}

\begin{itemize}
    \item \mecite{probdr}: Dimensionality Reduction as Probabilistic Inference, \textit{1$^{st}$ author, AABI \& Stancon 2023}\footnote{A version of this work was first submitted for review in February 2022 as ``A Unifying Probabilistic Perspective on Graph Latent Variable Models'' and was presented in poster form throughout that year. It introduced the core graphical model and a variational interpretation of the (t-)SNE and UMAP objectives. Independently, the variational idea appeared in the second preprint of \cite{assel}, released in June 2022. As these developments occurred in parallel, they are treated as concurrent work. To the best of our knowledge, our dual-probabilistic formulation of MCA has not appeared in prior literature.}. Contributions include all technical ideas, experiments, code and write-up, except for Appendix D of the paper (a mean-field view), which is not presented in this thesis. Francisco Vargas devoted a lot of time for proof-checking ideas and proof-reading the math. VL helped with proof-reading and provided feedback on the writing.
    \item \mecite{probdr2}: Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE, \textit{1$^{st}$ author, AABI 2025}. Contributions include all technical ideas, experiments, code and write-up.
\end{itemize}

\subsection*{Extended contributions (this forms \Cref{chp:probdr-nn}: Extensions of ProbDR)}

\begin{itemize}
    \item \mecite{probdr_t}: Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps: An Interpretation and Potential Improvements, \textit{1$^{st}$ author, SPIGM, UniReps @ NeurIPS Workshops 2025}. Contributions include all technical ideas, experiments, code and write-up.
\end{itemize}

\textcontrib{\subsection*{Other contributions throughout the thesis}}
Unpublished or ideas in the appendices that use original work are highlighted \textcontrib{grey}.
