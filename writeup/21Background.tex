\chapter{Background}
\label{part:i}

In the thesis, we aim to uncover the probabilistic models that underpin methods in representation learning. In the background, we establish a description of the probabilistic terminology used, and present an example of how probabilistic interpretations are constructed, which is needed for the presentation of the ideas in later chapters. In addition to this, we also contextualise the thesis by providing an overview of ideas in probabilistic representation learning, organised as \textbf{projective} and \textbf{generative} models. Yet a significant number of algorithms in the field lack explicit probabilistic interpretations; we provide a brief review of such algorithms. This absence raises a critical question that drives our inquiry: where should these algorithms sit given our categorisation and what, in modelling terms, do they do? We briefly mention case-studies (and expand on them in \cref{app:cases}) showing how the ideas of the background appear in scientific modelling, to lend credibility to the ideas presented.

In later chapters, we interpret the dimensionality reduction and neighbour embedding algorithms introduced in \cref{chp:bck-nea} as performing covariance estimation or edge-detection. All the while, as we see in our scientific examples, the algorithms constrain degrees of freedom by using specific estimators for statistics of the data. For example, some methods use the graph Laplacian as an implicit proxy for the data covariance, rather than using the standard estimator. Future work should focus on precisely what these estimators estimate in high-level terms.

The sections in this chapter are organised as follows. \Cref{chp:bck-prob} provides a background on the mathematical tools within probability and statistics useful for scientific modelling. \Cref{chp:bck-interp} shows how one can construct probabilistic interpretations from loss functions, and particularly, we see how variational interpretations can arise by studying the Griffin-Lim algorithm. In \cref{chp:bck-proj} and \cref{chp:gen-bck}, alongside providing an exposition of major ideas in probabilistic representation learning, we also mention examples in science, showing that estimators constrain degrees of freedom and/or showcase probabilistic models being successfully used in ways that perhaps contrasts with their intended use-case. The case-studies are provided as an \textbf{ideological prologue} to the rest of the thesis, but we leave a longer exposition of the ideas to the appendix. In \cref{chp:bck-nea}, we provide a background on commonly used dimensionality reduction methods and representation learning methods, which all use the notion of data-data similarity and ask the question: where should these algorithms sit within our taxonomy?
