\abstract{%

In this thesis, we introduce a coherent probabilistic framework for dimensionality reduction, and show how it can be used to understand aspects of wider representation learning. We motivate the search for a probabilistic paradigm by showing that in science, probabilistic models can be used to identify cancer-cells, enable conservation, accelerate climate science, and improve drug discovery, often in ways outside their intended use.
In scientific pipelines, various algorithms are used to generate representations of data for downstream analysis. How they relate to each other, however, in general, is not well understood. A unified understanding of the methods has the potential to unlock new methods for science.

We introduce one such unification, with the aim of contributing to statistical model understanding, by showing how a single probabilistic framework ``ProbDR'' underpins many classical dimensionality reduction methods. ProbDR presents the algorithms as inference algorithms of probabilistic models, highlighting what modelling and \GLS{semantic} assumptions these methods make, and how they are constrained. We present model transformations within ProbDR, showing that its different forms of model specification are equivalent. We then show that insights from model equivalences approximately yield new analytical inference algorithms. ProbDR also highlights the form of constraints necessary for latent variable models---the explicit usage of case-dependent similarity estimators.
% (e.g. the necessity for constraints, unidentifiability, the importance of initialisation, and local vs. global behaviour).
In the last part of the thesis, we show that ideas of the framework extend to other areas of representation learning, explaining aspects of self-supervised learning models and transformers, leading to improvements in their performance, validated empirically on language and vision tasks. This exemplifies that the insights of our probabilistic framework can be applied to areas not of primary focus. With these ideas, we take a step towards describing characteristics of a paradigm for representation learning.

% remove eg and relace with actual list as ow it comes across as an afterthought, no brackets
% temporal progression, using adverbs like consequently, firstly, subsequently
% the why, the stakes, emphasise and embolden, walk reader through
% embrace that its nontraditional
% other people don't do this, I take this macroscopic view
% doesn't start with thus, use therefore or something stronger

% don't be descriptive, make sure there's analysis, evaluates / improves upon
% clear signposting, reiterate the key argument that you're making

}